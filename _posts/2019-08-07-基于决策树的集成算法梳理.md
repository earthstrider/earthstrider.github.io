---
layout:     post
title:      基于决策树的集成算法梳理
subtitle:   
date:       2019-08-07
author:     猫不见了
comments:	true
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - Algorithm
---

## 集成学习

**集成学习概念**

西瓜书这样定义集成学习：集成学习通过构建并结合多个学习器来完成学习任务。

集成就是组合或者结合，就是将多个单一学习器（术语是个体学习器）通过某种方式组合在一起使用，以获得比单一学习器更高的学习性能（术语是泛化性能）。

将多个学习能力良莠不齐的单一学习器组合在一起，我们当然希望集成之后的学习器的学习性能比任一单一学习器的学习性能强，就像毛主席所说的众人拾柴火焰高，但真得能实现吗？会不会发生木桶效应或者一颗老鼠屎带坏一锅粥？这就需要我们研究集成学习算法以达到我们的目的。当然这里又可以引出一个自认为是哲学的问题，你更赞成英雄主义还是众人同心？



**个体学习器概念**

个体学习器是集成学习器的组成单元。当然在不同情况下这个词的称呼稍有不同，当集成学习器都是由多个相同个体学习器组成的时候，该个体学习器称为基学习器，例如决策树集成中全是决策树；当集成学习器都是由多个不同个体学习器组成的时候，该个体学习器称为组件学习器。



集成学习研究的核心是如何产生并结合"好而不同"的个体学习器，好代表个体学习器要有一定的"准确性"，不同代表个体学习器间具有差异，或者说"多样性"。



问题： Hoeffding不等式                       



**个体学习器生成策略**

Boosting

先从初始训练集训练出一个个体学习器，后续学习器更关注前一个学习器表现不好的样本，如此重复进行，直到学习器的数目达到指定的值T，最终将这T个个体学习器进行加权结合。



Bagging

使用相互有交叠的样本子集训练出不同的个体学习器，在对预测输出进行结合时，Bagging通常对分类任务使用简单的投票法，对回归任务使用简单平均法。

同：都是训练不同的个体学习器，然后对这些个体学习器进行结合。

异：因为要使个体学习器间具有差异，Boosting和Bagging采用的方式不同。



当基学习器是决策树时，使用Boosting方法生成的集成学习器称为提升决策树；使用Bagging方法生成的集成学习器称为随机森林，当然随机森林在Bagging样本扰动的基础上，加上属性扰动。



**个体学习器结合策略**

平均法

投票法

学习法



## 决策树

介绍随机森林之前，复(yu)习(xi)下决策树。




决策树学习采用的是自顶向下的递归方法，其基本思想是以**信息熵**为度量构造一颗熵值下降最快的树。理想情况是所有叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类。

下面介绍熵以及条件熵的定义。



**信息熵**

香龙把随机变量$X$的**熵**值$H$(希腊字母Eta)定义如下，$X$值域为${x_1,…,x_n}$：

$$
H(X)=E[I(X)]=E[-ln(P(X))]
$$

当取自有限的样本时，熵的公式可以表示为：

$$
H(X)=\sum p(x_i)I(x_i)=-\sum p(x_i)\log_bp(x_i)
$$

$b$通常取2，自然常数e，或10，相应熵的单位分别是bit、nat、Hart。

注意$\lim \limits_{p->0}p\log_bp=0$。

还可以定义事件$X$与$Y$分别取$x_i$和$y_j$时的**条件熵**为

$$
H(X|Y) = -\sum_{i,j}p(x_i, y_j)\log{\frac{p(x_i,y_j)}{p(y_j)}}
$$

条件熵可以理解为是衡量已知$Y$的前提下变量$X$的随机性。



问题：

$$
entropy=p_true*calcEntropy(target_true)+p_false*calcEntropy(target_false)
$$

如何利用熵来计算条件熵？



**经典例子**


![image-20190806193351718](http://ww3.sinaimg.cn/large/006tNc79ly1g5qa0yk217j30ds0dg44c.jpg)

![image-20190806194124579](http://ww2.sinaimg.cn/large/006tNc79ly1g5qa2qojnqj30pn0byjs7.jpg)

变量Play的信息熵如下：


$$
H(Play) = -[\frac{9}{14}*\log\frac{9}{14}]-[\frac{5}{14}*\log\frac{5}{14}]
$$

分别计算$$H(Play \vert Outlook)，H(Play \vert Temperature)，H(Play \vert Humidity)，H(Play \vert Windy)$$四个条件熵。

特征$$A$$对数据集$D$的信息增益

$$
g(D,A)=H(D)-H(D \vert A)
$$

这里$$H(D)$$就是$H(Play)$。选择信息增益最大的特征作为当前的分裂特征。

信息增益率$$g_r(D,A)=g_r(D,A)/H(A)$$



ID3算法（信息增益）

C4.5算法（信息增益率）

CART算法（基尼系数）



剪枝算法 - 过拟合



代码实现

```python

```



## Bagging





## 随机森林

下面重点介绍随机森林，包括器思想、优缺点、推广、实现与应用。



**随机森林思想**

随机森林是在bagging的行采样的基础上进行列采样，以使得组成随机森林的决策树间具有差异。由Hoeffding不等式可以得到，决策树间差异性越大，随机森林的学习能力越好。

行采样：模型从N条数据集中随机采样n条数据，一般情况下n取M的平方根大小，分别作为每一棵决策树的训练集。行采样保证了每棵决策树使用的训练集各不相同。列采样使决策树间具有差异性。

列采样：每一棵决策树都从M个特征中随机挑选m个特征作为节点分裂特征来计算，一般情况下m取M的平方根大小。列采样具体又分为两种方式，一种是全局列采样，即同一棵树的建树过程均采用同一批采样特征；另一种是局部列采样，即每一次节点分裂的时候均单独随机挑选m个特征进行扩展。列采样进一步增加决策树间的差异性。



**优缺点**



**推广**



**scikit-learn**

```python
class  sklearn.ensemble.RandomForestClassifier(
  n_estimators='warn',  # 森林中决策树的个数
  criterion='gini',     # 选取分裂特征时使用的度量方法
  max_depth=None, 
  min_samples_split=2,  # 内部节点包含的样本数量的最小值
  min_samples_leaf=1,   # 叶子节点含有样本数量的最小值
  min_weight_fraction_leaf=0.0, 
  max_features='auto', 
  max_leaf_nodes=None, 
  min_impurity_decrease=0.0, 
  min_impurity_split=None, 
  bootstrap=True, 
  oob_score=False, 
  n_jobs=None, 
  random_state=None, 
  verbose=0, 
  warm_start=False, 
  class_weight=None
)
```



**应用场景**





## Boosting

Boosting是一种常用的统计学习方法，AdaBoost是这种方法的经典算法。



**AdaBoost算法**

训练数据集$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$，其中$$x_i \in X  \subseteq B，y_i\in Y=\{-1,+1\}$$。

1. 初始化训练数据的权值分布

$$
D_1=(w_{11},...,w_{1i},...,w_{1N}), w_{1i}=\frac{1}{N},i=1,2,...,N
$$



2. 训练$$M$$个分类器。每个分类器使用的样本权值不同，前一个分类器错误判断的样本的权重会加强。
3. 构建基本分类器的线性组合。



**前向分布算法**

前向分布算法可以降低加法模型的优化复杂性。前从前往后，每一次在之前模型的基础上，仅优化当前的决策树模型。



损失函数

$$
(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma} \sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))
$$






## 提升树



**分类问题**

指数损失函数



**回归问题**

如果将输入空间$$X$$划分为$$J$$个互不相交的区域$$R_1,R_2,...,R_J$$，并且在每个区域上确定输出的常量$$c_j$$，那么树可表示为
$$
T(x;\theta)=\sum_{j=1}^J c_j I(x\in R_j)
$$


平方损失函数为：

$$
L(y,f_{m-1}(x)+T(x;\theta_m))
$$
$$
=[y-f_{m-1}(x)-T(x;\theta_m)]^2=[r-T(x;\theta_m)]^2
$$

这里$$r$$是当前模型拟合数据的残差。所以对回归问题的提升树算法来说，只需要简单拟合当前模型的残差。



> $$I$$代表指示函数，当输入为True时，输出为1，输入为False时，输出为0。





## GBDT

回归问题中，梯度提升树的算法与原始提升树算法的核心区别在于残差计算，由于原始提升树使用平方损失函数，所以可以直接计算残差；而梯度提升树针对一般损失函数，所以采用**损失函数的负梯度来近似求解残差**。即：

$$
r_{mi}=-[\frac{\partial L(y_i, f(x_i))partialart f(x_i)}]_{f(x)=f_{m-1}(x)}
$$


那么为什么可以这样近似？
$$
L(y,f(x))=L(y,y^{t-1}+f_t(x))
$$


> https://www.zhihu.com/question/60625492



**优缺点**

- 可以灵活处理各种类型的数据，包括连续值和离散值；
- 使用健壮的损失函数，对异常值的鲁棒性非常强，比如 Huber损失函数和Quantile损失函数；
- 充分考虑的每个分类器的权重。



- 弱学习器之间存在依赖关系，难以并行训练数据。



**sklearn**

GradientBoostingClassifier与GradientBoostingRegressor，主要参数如下：

- n_estimators
- learning_rate
- subsample
- init
- loss
- alpha




## XGB

extreme gradient boost

损失函数加入惩罚项，如下：

$$
Obj^{(t)} = \sum_{i=1}^n l(y_i,\hat y_i^{(t-1)})+\Omega(f_t)+constant
$$

其中惩罚项$$\Omega(f_t)=\gamma T+\frac{1}{2}\lambda \sum_{j=1}^T \omega_j^2$$。



泰勒展开如下：

$$
f(x+\Delta x)\simeq f(x)+f'(x)\Delta x+\frac{1}{2}f''(x)\Delta x^2
$$

定义$$g_i=\partial_{\hat y^{(t-1)}}l(y_i, \hat y^{(t-1)}),h_i=\partial_{\hat y^{(t-1)}}^2l(y_i, \hat y^{(t-1)})$$。

 

带入得到：

$$
Obj^{(t)}\simeq \sum_{i=1}^n [l(y_i, \hat y^{(t-1)})+g_i f_t(x_i)+\frac{1}{2}h_i f_t^2(x_i)]+\Omega(f_t)+constant
$$

最后优化的部分：

$$
\sum_{i=1}^n [g_i f_t(x_i)+\frac{1}{2}h_i f_t^2(x_i)]+\Omega(f_t)
$$


**sklearn参数**

- eta



> https://www.bilibili.com/video/av26088803/?p=3





## LightGBM

数据 模型 损失函数 学习算法


















## 参考

[决策树和随机森林-youtube视频](https://www.youtube.com/watch?v=EFaVlXWLYkQ)

[python决策树算法](http://blog.lisp4fun.com/2018/03/03/decision-tree)

[随机森林原理及Python3实现代码](https://blog.csdn.net/weixin_39449570/article/details/85061228)

附录A — 希腊字母发音对照表

 ![image-20190807181411579](http://ww1.sinaimg.cn/large/006tNc79ly1g5ra46gblbj30ey0da75f.jpg)
